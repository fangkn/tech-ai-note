
# python 学习列表

- Python编程 OK 
- python 关键库 ： NumPy（科学计算）、Pandas（数据处理和分析）、Matplotlib 和 Seaborn（数据可视化）

# 数学基础

 - 线性代数：理解向量、矩阵、张量、点积、矩阵乘法。这些是数据表示和模型运算的核心
 - 微积分：理解导数和梯度的概念。这是模型学习（梯度下降）的基础
 - 概率与统计：理解基础概率、条件概率、正态分布、均值和方差。这是理解模型不确定性和评估的基础

## 微积分

- 函数的概念
   - 自变量、因变量、映射思想
   - 常见函数：多项式、指数、对数、三角函数
- 极限（Limit）
  - 极限的定义与直觉（趋近的思想）
  - 左极限、右极限
  - 无穷大与无穷小
- 连续性（Continuity）
  - 函数在某点连续的条件
  - 不连续点的类型（间断、跳跃）
- 导数的定义 
  - 平均变化率 → 瞬时变化率
  - 几何意义：切线斜率
  - 物理意义：速度、加速度
- 求导法则
  - 和差、乘积、商法则
  - 链式法则（Chain Rule）
  - 反函数、隐函数求导
- 高阶导数
  - 二阶导、三阶导
  - 凸函数与凹函数（机器学习中用于优化判断）
- 微分的定义
  - 增量公式
  - 一阶近似（线性化）
  - 泰勒展开（Taylor Series）
- 不定积分
  - 反导函数
  - 基本积分公式
  - 换元法、分部积分法
- 定积分
  - 面积的定义与近似（矩形法、黎曼和）
  - 牛顿–莱布尼茨公式（微积分基本定理）
- 多元函数与偏导数
  - 偏导数的定义与意义
  - 全微分
  - 梯度（Gradient）与方向导数 
- 梯度与最优化
  - 梯度向量的方向意义
  - 极值点、驻点、Hessian矩阵
  - 梯度下降法（Gradient Descent）
- 重积分与体积  
  - 二重积分、三重积分
  - 计算体积、质量分布等
- 其他

  - 向量值函数与雅可比矩阵
  - 梯度场与散度、旋度（可选）
  - 概率密度函数与积分
  - 优化与收敛性分析


 # 入门经典机器学习

- 核心概念与流程 
 - 器学习类型：区分监督学习（分类、回归）、无监督学习（聚类、关联\降维）半监督学习(混合学习) 和强化学习，自监督学习
    - [监督学习、无监督学习、半监督学习、强化学习、自监督学习](https://zhuanlan.zhihu.com/p/667916299)
    - 监督学习与无监督学习的本质区别就在于用来训练的数据是否已经被标注。 
    - 半监督学习就是两者的混用，只标记数据集中的一部分，而不是标记整个数据集，然后用半监督学习方式来学习。
    - 强化学习（Reinforcement Learning）是一种机器学习技术，它基于反馈的学习方法，对算法执行的正确和不正确行为分别进行奖励和惩罚的制度，目的是使算法获得最大的累积奖励。
    - “强化”学习的要素需要明确：
      - 代理人，Agent：一个我们试图学习的实体（即玩家在游戏中所使用的角色）；
      - 环境，Environment：代理人所处的环境（游戏所设置的游戏世界设定）；
      - 状态，State：代理人在环境中获得自己当前状态的各种信息；
      - 行动，Actions：代理人在环境中所执行的与环境交互的各种动作（马里奥游戏中的行走、跑步、跳跃等等）；
      - 奖励，Reward：代理人从环境中获得的行动反馈（在马里奥的游戏里，即为正确的行动增加的积分/硬币，是一个积极的奖励。因落入陷阱或被怪物吃掉而丢失积分，或损失一条“命”，则是一个消极的奖励）；
      - 策略，Policy：根据代理人当前的状态决定一个合适的决策，以最大化地在未来某个时间段内获得正面报酬，最小化获得负面的惩罚；
      - 价值函数， Value function：决定什么才是对代理人是有益的。 
   - 自监督学习不需要人工标注训练数据，它的模型主要训练从大规模的无监督数据中挖掘能够应用于自身的监督信息，从而从输入的一部分数据中去学习另一部分。  
   - Embedding （嵌入） 就是具体的“东西”成带语义的数字串，让电脑能像我们一样判断 “谁和谁更像”
   - 比较经典的 Enbedding 模型
     - 文本 Embedding 经典模型
       - Word2Vec 
       - GloVe
       - BERT
       - Sentence-BERT
      - 图像 Embedding 经典模型
        - CNN 类预训练模型
        - MoCo/SimCLR

  Embedding 模型对比
| 模型类别       | 模型名称                  | 提出时间&机构                | 核心原理                                                                 | 核心优势                                                                 | 局限性                                                                 | 典型应用场景                                                           |
| -------------- | ------------------------- | ---------------------------- | ------------------------------------------------------------------------ | ------------------------------------------------------------------------ | ---------------------------------------------------------------------- | ---------------------------------------------------------------------- |
| 文本Embedding  | Word2Vec                  | 2013年，谷歌                 | 基于神经网络，通过CBOW（上下文预测中心词）或Skip-gram（中心词预测上下文）学习词向量 | 训练速度快，适合大规模语料，能捕捉词语间基础语义关联（如“国王-男人+女人≈女王”） | 静态词向量，无法处理多义词；对低频词、未见词支持差，上下文理解受固定窗口限制 | 早期文本分类、推荐系统、基础语义关系挖掘                               |
| 文本Embedding  | GloVe                     | 2014年，斯坦福大学           | 构建词的共现矩阵，通过矩阵分解学习向量，结合全局词频统计和局部上下文关系       | 兼顾全局统计信息与局部语义，对语料库规模依赖较小，语义稳定性强             | 静态词向量，无法适配多义词场景；对稀有词表征效果一般                     | 中小型语料的文本处理、对上下文变化不敏感的文本匹配任务                   |
| 文本Embedding  | FastText                  | 2016年，脸书                 | 将单词拆分为子词（n-gram），词向量为所有子词向量的组合                       | 对未见词泛化能力强，能捕捉单词形态特征（如前缀、后缀），对拼写错误鲁棒性强     | 训练速度较慢，对中文等非形态化语言的提升效果有限                         | 快速文本分类、含大量生僻词的多语言文本处理                               |
| 文本Embedding  | BERT                      | 2018年，谷歌                 | 基于Transformer架构，通过掩码语言模型和下一句预测，生成动态词向量             | 解决多义词问题，能深度捕捉长文本语义，适配复杂语义理解任务                 | 参数量大，推理速度慢；对低资源语言支持不足                               | 问答系统、情感分析、精准语义搜索                                         |
| 文本Embedding  | Sentence-BERT             | 2019年，研究者Nils Reimers等 | 基于BERT的孪生网络结构，通过对比学习生成固定长度的句子向量                   | 解决BERT句向量语义损失问题，相似度计算效率高，模型体积小于BERT               | 依赖BERT预训练模型，对极端长尾语义的表征能力有限                         | 句子相似度对比、语义检索、文本聚类                                       |
| 图像Embedding  | ResNet                    | 2015年，微软研究院           | 通过残差连接解决深层CNN梯度消失问题，用卷积层提取图像局部特征，全连接层生成全局嵌入向量 | 擅长捕捉图像空间结构和视觉特征（边缘、纹理），迁移学习效果好，模型成熟       | 对图像高层语义抽象能力较弱，难以直接适配跨模态任务                         | 图像分类、相似图像检索、视觉问答基础特征提取                             |
| 图像Embedding  | ViT（Vision Transformer） | 2020年，谷歌                 | 将图像分割为多个补丁，输入Transformer编码器，引入位置编码捕捉全局关系           | 利用自注意力机制捕捉图像全局信息，在大规模数据集上性能超越传统CNN           | 对小数据集泛化能力差，需要大量数据增强，依赖大规模预训练                   | 医学影像分析、高清图像特征提取、图像分类进阶任务                           |
| 图像Embedding  | DINO-v2                   | 2023年，元宇宙               | 基于ViT架构，采用自监督+知识蒸馏（学生-教师网络）训练，学习图像像素级特征     | 通用视觉特征表征能力强，可适配图像分类、分割等多种视觉任务，特征质量高       | 训练依赖高质量大规模数据集，计算成本较高                                 | 高精度图像检索、图像语义分割、视觉特征匹配                               |
| 跨模态Embedding | CLIP                      | 2021年，OpenAI               | 通过对比学习对齐图像与文本特征，图像编码器和文本编码器生成同空间嵌入向量       | 支持零样本图像分类，语义对齐能力强，可直接计算图文相似度                   | 训练需数十亿图文对，计算成本极高；对图像细节特征的表征不如纯视觉模型       | 图文检索、跨模态内容匹配、零样本图像标注                                 |
| 跨模态Embedding | BLIP-2                    | 2023年，Salesforce           | 引入Q-Former对齐预训练视觉模型与语言模型的特征空间，复用CLIP等模型的预训练成果 | 训练效率高，兼顾视觉细节与文本语义，适配复杂跨模态交互任务                 | 依赖多个预训练模型，模型链路较长，部署门槛稍高                             | 图像描述生成、跨模态对话、视觉问答（VQA）进阶任务                         |
 - 标准工作流 ：数据收集与清洗 -> 特征工程 -> 模型训练 -> 模型评估 -> 调参优化 -> 模型部署 这个流程
 - 关键概念：过拟合 vs. 欠拟合、偏差-方差权衡、训练集/验证集/测试集划分、交叉验证. 

 # Scikit-learn库并实践经典模型

Scikit-learn 是Python中最重要、最经典的机器学习库，封装了几乎所有经典算法
 
学习顺序：
- 线性回归、逻辑回归：最好的起点，易于理解。
- 决策树：直观，为后续的集成方法打基础。
- 支持向量机（SVM）：强大的分类器。
- k-近邻（k-NN）：简单的惰性学习算法。
- 无监督学习：k-Means 聚类，PCA 降维。

Kaggle（一个数据科学竞赛平台）找一些入门比赛，比如 Titanic: Machine Learning from Disaster，完整地走一遍工作流。

# 深度学习

##  深度学习框架

- PyTorch 或 TensorFlow/Keras 二选一。目前更推荐PyTorch给初学者，因为它更Pythonic，设计和调试更直观。工业界两者都用，但学术界PyTorch占主导。
、
- 核心概念：张量（Tensor）、自动求导（Autograd）、神经网络模块（torch.nn）。

## 掌握神经网络基础

- 连接网络（FCN）：最基本的神经网络。
- 激活函数：ReLU, Sigmoid, Tanh。
- 损失函数：交叉熵损失、均方误差。
- 优化器：随机梯度下降（SGD）、Adam。
- 实践项目：在Fashion-MNIST这样的标准数据集上训练一个全连接网络进行图像分类

## 征服卷积神经网络（CNN）

- 这是计算机视觉的基石。回顾我们刚才讨论的AlexNet论文，你会理解得更加深刻。
- 核心概念：卷积层、池化层、Dropout、批归一化。
- 实践项目：在CIFAR-10数据集上搭建和训练一个简单的CNN（比如LeNet-5或一个迷你版VGG），性能远超全连接网络。

#专项深化与项目实践

选择方向深化

- 计算机视觉（CV）：目标检测（YOLO）、图像分割、生成模型（GANs, Diffusion）。
- 自然语言处理（NLP）：词向量（Word2Vec）、循环神经网络（RNN/LSTM）、Transformer架构、BERT、大语言模型（LLMs）。
- 推荐系统：协同过滤、矩阵分解、深度学习推荐模型。

疯狂做项目

这是巩固知识、建立作品集的最佳方式。
来源：Kaggle竞赛、天池竞赛、从生活中发现问题（例如，爬取数据分析某个社会现象）、复现论文中的简单模型。
目标：每个项目都应有清晰的文档和代码，放在你的GitHub上，这是你未来求职的宝贵财富。

# 资源推荐

吴恩达《机器学习》（Coursera）：经典入门，奠定理论基础。
吴恩达《深度学习专项课程》（Coursera）：系统学习深度学习。
李沐《动手学深度学习》（书籍/课程）：强烈推荐！结合理论和代码实践，使用PyTorch。

《Python数据科学手册》
《统计学习方法》（李航）—— 理论较深
《动手学深度学习》（Aston Zhang, Zachary C. Lipton, Mu Li, Alexander J. Smola）—— 最佳实践书

社区与平台：

Kaggle：学习、竞赛、获取数据的圣地。
GitHub：学习别人的代码，管理自己的项目。
Stack Overflow：解决编程问题。
知乎， CSDN：中文社区，有很多优质分享。


## 其他

[怎么成为一个 ai agent 工程师](https://www.zhihu.com/question/1936375725931361485/answer/1948826620803657995)

[Claude 的官方教程](https://anthropic.skilljar.com/claude-with-the-anthropic-api)
