
  Embedding 模型对比
  
| 模型类别       | 模型名称                  | 提出时间&机构                | 核心原理                                                                 | 核心优势                                                                 | 局限性                                                                 | 典型应用场景                                                           |
| -------------- | ------------------------- | ---------------------------- | ------------------------------------------------------------------------ | ------------------------------------------------------------------------ | ---------------------------------------------------------------------- | ---------------------------------------------------------------------- |
| 文本Embedding  | Word2Vec                  | 2013年，谷歌                 | 基于神经网络，通过CBOW（上下文预测中心词）或Skip-gram（中心词预测上下文）学习词向量 | 训练速度快，适合大规模语料，能捕捉词语间基础语义关联（如“国王-男人+女人≈女王”） | 静态词向量，无法处理多义词；对低频词、未见词支持差，上下文理解受固定窗口限制 | 早期文本分类、推荐系统、基础语义关系挖掘                               |
| 文本Embedding  | GloVe                     | 2014年，斯坦福大学           | 构建词的共现矩阵，通过矩阵分解学习向量，结合全局词频统计和局部上下文关系       | 兼顾全局统计信息与局部语义，对语料库规模依赖较小，语义稳定性强             | 静态词向量，无法适配多义词场景；对稀有词表征效果一般                     | 中小型语料的文本处理、对上下文变化不敏感的文本匹配任务                   |
| 文本Embedding  | FastText                  | 2016年，脸书                 | 将单词拆分为子词（n-gram），词向量为所有子词向量的组合                       | 对未见词泛化能力强，能捕捉单词形态特征（如前缀、后缀），对拼写错误鲁棒性强     | 训练速度较慢，对中文等非形态化语言的提升效果有限                         | 快速文本分类、含大量生僻词的多语言文本处理                               |
| 文本Embedding  | BERT                      | 2018年，谷歌                 | 基于Transformer架构，通过掩码语言模型和下一句预测，生成动态词向量             | 解决多义词问题，能深度捕捉长文本语义，适配复杂语义理解任务                 | 参数量大，推理速度慢；对低资源语言支持不足                               | 问答系统、情感分析、精准语义搜索                                         |
| 文本Embedding  | Sentence-BERT             | 2019年，研究者Nils Reimers等 | 基于BERT的孪生网络结构，通过对比学习生成固定长度的句子向量                   | 解决BERT句向量语义损失问题，相似度计算效率高，模型体积小于BERT               | 依赖BERT预训练模型，对极端长尾语义的表征能力有限                         | 句子相似度对比、语义检索、文本聚类                                       |
| 图像Embedding  | ResNet                    | 2015年，微软研究院           | 通过残差连接解决深层CNN梯度消失问题，用卷积层提取图像局部特征，全连接层生成全局嵌入向量 | 擅长捕捉图像空间结构和视觉特征（边缘、纹理），迁移学习效果好，模型成熟       | 对图像高层语义抽象能力较弱，难以直接适配跨模态任务                         | 图像分类、相似图像检索、视觉问答基础特征提取                             |
| 图像Embedding  | ViT（Vision Transformer） | 2020年，谷歌                 | 将图像分割为多个补丁，输入Transformer编码器，引入位置编码捕捉全局关系           | 利用自注意力机制捕捉图像全局信息，在大规模数据集上性能超越传统CNN           | 对小数据集泛化能力差，需要大量数据增强，依赖大规模预训练                   | 医学影像分析、高清图像特征提取、图像分类进阶任务                           |
| 图像Embedding  | DINO-v2                   | 2023年，元宇宙               | 基于ViT架构，采用自监督+知识蒸馏（学生-教师网络）训练，学习图像像素级特征     | 通用视觉特征表征能力强，可适配图像分类、分割等多种视觉任务，特征质量高       | 训练依赖高质量大规模数据集，计算成本较高                                 | 高精度图像检索、图像语义分割、视觉特征匹配                               |
| 跨模态Embedding | CLIP                      | 2021年，OpenAI               | 通过对比学习对齐图像与文本特征，图像编码器和文本编码器生成同空间嵌入向量       | 支持零样本图像分类，语义对齐能力强，可直接计算图文相似度                   | 训练需数十亿图文对，计算成本极高；对图像细节特征的表征不如纯视觉模型       | 图文检索、跨模态内容匹配、零样本图像标注                                 |
| 跨模态Embedding | BLIP-2                    | 2023年，Salesforce           | 引入Q-Former对齐预训练视觉模型与语言模型的特征空间，复用CLIP等模型的预训练成果 | 训练效率高，兼顾视觉细节与文本语义，适配复杂跨模态交互任务                 | 依赖多个预训练模型，模型链路较长，部署门槛稍高                             | 图像描述生成、跨模态对话、视觉问答（VQA）进阶任务                         |
 